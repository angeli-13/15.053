{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15.053: Gradient Search\n",
    "\n",
    "### Step 1: What is Gradient Search? \n",
    "Try running the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.25.2)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.7.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mac-air/Library/Python/3.10/lib/python/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mac-air/Library/Python/3.10/lib/python/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mac-air/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Gradient search, often referred to in the context of optimization, is a method used to find local minima or maxima of a function. This technique is particularly common in machine learning and artificial intelligence for training models, such as neural networks. The key concept behind gradient search is the use of gradients to guide the search process.\n",
       "\n",
       "Here's a step-by-step explanation:\n",
       "\n",
       "1. **Objective Function**: Gradient search is used to optimize an objective function, which is a formula that the algorithm seeks to minimize or maximize. In machine learning, this could be a loss function that measures how well a model is performing.\n",
       "\n",
       "2. **Gradient Calculation**: The gradient of the objective function is calculated. In mathematical terms, the gradient is a vector of partial derivatives. It points in the direction of the steepest ascent of the function. For minimization problems, the opposite direction of the gradient is considered.\n",
       "\n",
       "3. **Starting Point**: The algorithm starts with an initial guess. This is a starting point in the function's domain (like initial weights in a neural network).\n",
       "\n",
       "4. **Iterative Process**: Gradient search proceeds iteratively. In each step, it updates the current point in the domain of the function. The update is based on the gradient calculated at the current point.\n",
       "\n",
       "5. **Learning Rate**: The size of the step taken in the direction of the gradient is determined by a parameter known as the learning rate. A small learning rate makes the algorithm more precise but slower, whereas a large learning rate speeds up the process but can overshoot the minimum or maximum.\n",
       "\n",
       "6. **Convergence**: The process is repeated until the algorithm converges, meaning that the changes in the function value or the changes in the variable values become very small, indicating that a local minimum or maximum is found.\n",
       "\n",
       "7. **Challenges**: Gradient search can face challenges like getting stuck in local minima (or maxima) rather than finding the global minimum (or maximum), especially in complex, non-convex functions. Adjusting the learning rate and using advanced techniques like momentum or adaptive learning rates (like in Adam optimizer) can help alleviate some of these issues.\n",
       "\n",
       "Gradient search is the backbone of many optimization algorithms, including the popular Gradient Descent used in machine learning. The concepts of gradient search are fundamental to understanding how algorithms make incremental improvements towards optimal solutions in various fields of computational science."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explain gradient search to me \n",
    "from IPython.display import display_markdown\n",
    "explanation = open('explanation.txt').read()\n",
    "display_markdown(explanation, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Visualizing gradient descent--with a 1-variable function\n",
    "\n",
    "Using the function: $x^2 + 10sin(x)$, use gradient descent to find the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gradient_search(start_x, learning_rate, f_prime):\n",
    "    \"\"\" Perform gradient search \"\"\"\n",
    "    x = start_x\n",
    "    grad = f_prime(x)\n",
    "    x -= learning_rate * grad\n",
    "    return x\n",
    "\n",
    "def grad_search_visualization(initial_guess, f, f_prime, learning_rate, iteration, x_range=np.linspace(0,0,0)):\n",
    "    x_values = [initial_guess]\n",
    "    \n",
    "    current_x = x_values[-1]\n",
    "    current_y = f(current_x)\n",
    "    grad = f_prime(current_x)\n",
    "    next_guess = current_x - learning_rate * grad\n",
    "    next_point = (next_guess, f(next_guess))\n",
    "\n",
    "    # Plotting the function\n",
    "    y_values = f(x_range)\n",
    "    plt.plot(x_range, y_values, label='Function')\n",
    "    plt.scatter(current_x, current_y, color='red', marker='o', label='Current Guess')\n",
    "    plt.axhline(0, color='black', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Finding the intersection point with the x-axis\n",
    "    plt.scatter(next_guess, f(next_guess), color='green', marker='x', label=f'Next Guess at ({next_guess:.4f}, {f(next_guess):.4f})')\n",
    "    \n",
    "    plt.title(f'Gradient Search Visualization - Iteration {iteration}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.xlim(-5, 5)\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be85d0e9006a43939c71360c4735e1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next Step', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181932f166074b248660c23cea685f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing gradient search for a 1-variable function\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def f(x):\n",
    "    \"\"\" Define the function to optimize \"\"\"\n",
    "    return x**2 + 10*np.sin(x)\n",
    "\n",
    "def f_prime(x):\n",
    "    \"\"\" Compute the derivative (gradient) of the function \"\"\"\n",
    "    return 2*x + 10*np.cos(x)\n",
    "\n",
    "# Parameters\n",
    "inital_guess = 1.0  # Starting point\n",
    "learning_rate = 0.1  # Step size -- CHANGE ME \n",
    "n_steps = 10  # Number of steps\n",
    "\n",
    "# Create a range of x values for plotting\n",
    "loop_count = 1\n",
    "graph_size = np.linspace(-5, 5, 100) # Change size as you go \n",
    "\n",
    "def increment_loop_count(_):\n",
    "    global loop_count\n",
    "    global inital_guess\n",
    "    global next_guess\n",
    "    loop_count += 1\n",
    "    next_guess = gradient_search(inital_guess, learning_rate, f_prime)\n",
    "    inital_guess = next_guess\n",
    "    update_display()\n",
    "\n",
    "def update_display():\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Step Number {loop_count}\")\n",
    "        grad_search_visualization(inital_guess, f, f_prime, learning_rate, loop_count, graph_size)\n",
    "\n",
    "# Create a button and display it\n",
    "button = widgets.Button(description=\"Next Step\")\n",
    "button.on_click(increment_loop_count)\n",
    "\n",
    "# Create an output widget to display the loop count\n",
    "out = widgets.Output()\n",
    "\n",
    "# Display the button and output widget\n",
    "display(button, out)\n",
    "\n",
    "# Display initial loop count\n",
    "update_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Visualizing Gradient Descent in 2D\n",
    "\n",
    "function: $f(x,y) = (x-2)^2 + (x-2y)^2$\n",
    "\n",
    "initial guess: $(x,y) = (0,0)$\n",
    "\n",
    "At what point is f(x,y) at it's minimum? Find it with gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_search_2d(initial_guess, learning_rate, grad):\n",
    "    \"\"\" Perform gradient search \"\"\"\n",
    "    gradient = grad(initial_guess[0], initial_guess[1])\n",
    "    steepest_descent = -1* gradient\n",
    "    next_point = initial_guess + learning_rate * steepest_descent\n",
    "    return next_point\n",
    "\n",
    "def grad_search_visualization_2d(prev_guess_x, prev_guess_y, f, f_prime, learning_rate, iteration):\n",
    "    current_x = prev_guess_x[-1]\n",
    "    current_y = prev_guess_y[-1]\n",
    "    next_point = gradient_search_2d(np.array([prev_guess_x[-1], prev_guess_y[-1]]), learning_rate, f_prime)\n",
    "    \n",
    "    # plt.plot(x_range, y_values, label='Function')\n",
    "    plt.scatter(prev_guess_x, prev_guess_y, color='red', marker='o', label='Previous Point')\n",
    "    plt.axhline(0, color='black', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Finding the intersection point with the x-axis\n",
    "    plt.scatter(next_point[0], next_point[1], color='green', marker='x', label='Next Guess f(x\\', y\\')={:.4f}'.format(f(next_point[0], next_point[1])))\n",
    "    \n",
    "    plt.title('Gradient Search Visualization: f(x\\',y\\') = {:.4f}'.format(f(next_point[0], next_point[1])))\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.xlim(0, 3)\n",
    "    plt.ylim(0, 2)\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df417c63eb2f44dba4b7c199541b9385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next Step', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88066e19699242b6be31cf4d144ca84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def function(x, y):\n",
    "    \"\"\" Define the 2-variable function \"\"\"\n",
    "    return (x-2)**2 + (x-2*y)**2\n",
    "\n",
    "def gradient(x, y):\n",
    "    \"\"\" Compute the gradient of the function \"\"\"\n",
    "    return np.array([4*x-4*y-4, -4*x+8*y])\n",
    "\n",
    "inital_guess = np.array([0,0]) # Play with this initial guess\n",
    "prev_guesses_x = [0,]\n",
    "prev_guesses_y = [0,]\n",
    "learning_rate = 0.05 # Change this learning rate\n",
    "next_guess = None\n",
    "\n",
    "loop_count = 1\n",
    "graph_size = np.linspace(-2.5, 2, 100)\n",
    "\n",
    "def increment_loop_count(_):\n",
    "    global loop_count\n",
    "    global inital_guess\n",
    "    global next_guess\n",
    "    global prev_guesses_x\n",
    "    global prev_guesses_y\n",
    "    loop_count += 1\n",
    "    next_guess = gradient_search_2d(inital_guess, learning_rate, gradient)\n",
    "    inital_guess = next_guess\n",
    "    prev_guesses_x.append(inital_guess[0])\n",
    "    prev_guesses_y.append(inital_guess[1])\n",
    "    update_display()\n",
    "\n",
    "def update_display():\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Step Number {loop_count}\")\n",
    "        grad_search_visualization_2d(prev_guesses_x, prev_guesses_y, function, gradient, learning_rate, loop_count)\n",
    "\n",
    "# Create a button and display it\n",
    "button = widgets.Button(description=\"Next Step\")\n",
    "button.on_click(increment_loop_count)\n",
    "\n",
    "# Create an output widget to display the loop count\n",
    "out = widgets.Output()\n",
    "\n",
    "# Display the button and output widget\n",
    "display(button, out)\n",
    "\n",
    "# Display initial loop count\n",
    "update_display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
